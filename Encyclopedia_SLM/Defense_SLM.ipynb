{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d366dadf-6e6b-4ee0-93df-f0c21b4afe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (4.53.0)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (3.6.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (1.8.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\ajinkya\\appdata\\roaming\\python\\python312\\site-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "WARNING:tensorflow:From C:\\Users\\Ajinkya\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "GPU Memory: 6.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Install all required libraries including PDF processing\n",
    "!pip install transformers torch datasets accelerate\n",
    "!pip install PyPDF2 pandas numpy matplotlib tqdm\n",
    "!pip install scikit-learn\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    GPT2Config,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "import PyPDF2\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5798ce-9843-4c33-875b-d8e22cfdbe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'defense_dataset' folder - add your PDF, CSV, TXT, JSON files here\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def create_defense_pdf_chunks(text, source_file):\n",
    "    \"\"\"Create defense-optimized text chunks from PDF content\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Clean the text\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    \n",
    "    # Split by sentences for better chunking\n",
    "    sentences = text.split('. ')\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if len(current_chunk + sentence) < 400:  # Optimal chunk size for defense content\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            if len(current_chunk.strip()) > 100:  # Only keep substantial chunks\n",
    "                formatted_chunk = f\"Defense Document ({source_file}): {current_chunk.strip()}\"\n",
    "                chunks.append(formatted_chunk)\n",
    "            current_chunk = sentence + \". \"\n",
    "    \n",
    "    # Add the final chunk\n",
    "    if len(current_chunk.strip()) > 100:\n",
    "        formatted_chunk = f\"Defense Document ({source_file}): {current_chunk.strip()}\"\n",
    "        chunks.append(formatted_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_complete_defense_dataset(dataset_path):\n",
    "    \"\"\"Process ALL formats: PDF, CSV, TXT, JSON\"\"\"\n",
    "    all_training_data = []\n",
    "    supported_formats = {'.pdf', '.csv', '.txt', '.json', '.jsonl'}\n",
    "    \n",
    "    print(\"Processing complete defense dataset (including PDFs)...\")\n",
    "    \n",
    "    stats = {\n",
    "        'total_files': 0,\n",
    "        'pdf_files': 0,\n",
    "        'csv_files': 0,\n",
    "        'txt_files': 0,\n",
    "        'json_files': 0,\n",
    "        'total_examples': 0\n",
    "    }\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in tqdm(files, desc=\"Processing all dataset files\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_ext = os.path.splitext(file)[1].lower()\n",
    "            \n",
    "            if file_ext not in supported_formats:\n",
    "                continue\n",
    "                \n",
    "            stats['total_files'] += 1\n",
    "            \n",
    "            try:\n",
    "                if file_ext == '.pdf':\n",
    "                    stats['pdf_files'] += 1\n",
    "                    text = extract_text_from_pdf(file_path)\n",
    "                    if text.strip():\n",
    "                        pdf_chunks = create_defense_pdf_chunks(text, file)\n",
    "                        all_training_data.extend(pdf_chunks)\n",
    "                        print(f\"‚úì Processed PDF: {file} ({len(pdf_chunks)} chunks)\")\n",
    "                    else:\n",
    "                        print(f\"‚ö† Empty PDF: {file}\")\n",
    "                \n",
    "                elif file_ext == '.csv':\n",
    "                    stats['csv_files'] += 1\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    csv_data = process_defense_csv(df, file)\n",
    "                    all_training_data.extend(csv_data)\n",
    "                    print(f\"‚úì Processed CSV: {file} ({len(csv_data)} examples)\")\n",
    "                \n",
    "                elif file_ext == '.txt':\n",
    "                    stats['txt_files'] += 1\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                        content = f.read()\n",
    "                        txt_data = process_defense_txt(content, file)\n",
    "                        all_training_data.extend(txt_data)\n",
    "                        print(f\"‚úì Processed TXT: {file} ({len(txt_data)} examples)\")\n",
    "                \n",
    "                elif file_ext in ['.json', '.jsonl']:\n",
    "                    stats['json_files'] += 1\n",
    "                    json_data = process_defense_json(file_path, file)\n",
    "                    all_training_data.extend(json_data)\n",
    "                    print(f\"‚úì Processed JSON: {file} ({len(json_data)} examples)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚úó Error processing {file}: {e}\")\n",
    "    \n",
    "    stats['total_examples'] = len(all_training_data)\n",
    "    \n",
    "    print(f\"\\nüìä Complete Dataset Processing Summary:\")\n",
    "    print(f\"   Total files: {stats['total_files']}\")\n",
    "    print(f\"   PDF files: {stats['pdf_files']}\")\n",
    "    print(f\"   CSV files: {stats['csv_files']}\")\n",
    "    print(f\"   TXT files: {stats['txt_files']}\")\n",
    "    print(f\"   JSON files: {stats['json_files']}\")\n",
    "    print(f\"   Training examples: {stats['total_examples']:,}\")\n",
    "    \n",
    "    return all_training_data, stats\n",
    "\n",
    "def process_defense_csv(df, source_file):\n",
    "    \"\"\"Process CSV files for defense training data\"\"\"\n",
    "    training_examples = []\n",
    "    \n",
    "    # Common column names for defense data\n",
    "    text_columns = ['text', 'content', 'description', 'definition', 'explanation', 'data', 'term', 'meaning']\n",
    "    \n",
    "    # Find the best text column\n",
    "    text_col = None\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            text_col = col\n",
    "            break\n",
    "    \n",
    "    if text_col is None:\n",
    "        # Use first text column if no standard column found\n",
    "        text_cols = df.select_dtypes(include=['object']).columns\n",
    "        if len(text_cols) > 0:\n",
    "            text_col = text_cols[0]\n",
    "    \n",
    "    if text_col:\n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.notna(row[text_col]) and len(str(row[text_col]).strip()) > 20:\n",
    "                text = str(row[text_col]).strip()\n",
    "                \n",
    "                # Create multiple training formats\n",
    "                formats = [\n",
    "                    f\"Defense Encyclopedia: {text}\",\n",
    "                    f\"Military Knowledge: {text}\",\n",
    "                    f\"Defense Information: {text}\",\n",
    "                ]\n",
    "                \n",
    "                # Add term-definition format if applicable\n",
    "                if 'term' in df.columns and pd.notna(row.get('term')):\n",
    "                    term = str(row['term']).strip()\n",
    "                    formats.append(f\"What is {term}? {text}\")\n",
    "                \n",
    "                training_examples.extend(formats)\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "def process_defense_txt(content, source_file):\n",
    "    \"\"\"Process TXT files for defense training data\"\"\"\n",
    "    training_examples = []\n",
    "    \n",
    "    # Split by paragraphs or sections\n",
    "    sections = content.split('\\n\\n')\n",
    "    \n",
    "    for section in sections:\n",
    "        section = section.strip()\n",
    "        if len(section) > 100:  # Only keep substantial content\n",
    "            # Create defense-formatted training examples\n",
    "            formats = [\n",
    "                f\"Defense Document: {section}\",\n",
    "                f\"Military Reference: {section}\",\n",
    "                f\"Defense Knowledge Base: {section}\",\n",
    "            ]\n",
    "            training_examples.extend(formats)\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "def process_defense_json(file_path, source_file):\n",
    "    \"\"\"Process JSON/JSONL files for defense training data\"\"\"\n",
    "    training_examples = []\n",
    "    \n",
    "    try:\n",
    "        if file_path.endswith('.jsonl'):\n",
    "            # Line-delimited JSON\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data = json.loads(line.strip())\n",
    "                        examples = extract_json_examples(data)\n",
    "                        training_examples.extend(examples)\n",
    "        else:\n",
    "            # Regular JSON\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        examples = extract_json_examples(item)\n",
    "                        training_examples.extend(examples)\n",
    "                else:\n",
    "                    examples = extract_json_examples(data)\n",
    "                    training_examples.extend(examples)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing JSON {source_file}: {e}\")\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "def extract_json_examples(data):\n",
    "    \"\"\"Extract training examples from JSON data\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        # Look for common text fields\n",
    "        text_fields = ['text', 'content', 'description', 'definition', 'explanation', 'question', 'answer']\n",
    "        \n",
    "        for field in text_fields:\n",
    "            if field in data and isinstance(data[field], str) and len(data[field].strip()) > 20:\n",
    "                text = data[field].strip()\n",
    "                examples.append(f\"Defense Data: {text}\")\n",
    "                \n",
    "                # Create Q&A format if applicable\n",
    "                if field in ['definition', 'explanation', 'answer']:\n",
    "                    examples.append(f\"Question: What is this about?\\nAnswer: {text}\")\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Create defense dataset folder\n",
    "os.makedirs(\"defense_dataset\", exist_ok=True)\n",
    "print(\"Created 'defense_dataset' folder - add your PDF, CSV, TXT, JSON files here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb7f893-9817-4c07-95b6-29ba855e2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized gpt2 with 124,439,808 parameters\n"
     ]
    }
   ],
   "source": [
    "class DefenseGPT2:\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"Initialize the defense-focused GPT-2 model\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Add padding token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        self.model.to(device)\n",
    "        \n",
    "        print(f\"Initialized {model_name} with {self.model.num_parameters():,} parameters\")\n",
    "    \n",
    "    def generate_defense_response(self, prompt, max_length=200, temperature=0.7):\n",
    "        \"\"\"Generate defense-related text\"\"\"\n",
    "        # Format prompt for defense context\n",
    "        formatted_prompt = f\"Defense Encyclopedia: {prompt}\\nAnswer:\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer.encode(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "        \n",
    "        # Decode and clean response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response.replace(formatted_prompt, \"\").strip()\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize the model\n",
    "defense_gpt = DefenseGPT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c763f4-cc37-485f-ba44-6524e96dd0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete defense dataset (including PDFs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:   6%|‚ñå         | 1/18 [00:25<07:06, 25.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: 20140808_MOD_Acronyms_and_Abbreviations.pdf (31 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  11%|‚ñà         | 2/18 [00:48<06:21, 23.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: ADB280764.pdf (4513 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  17%|‚ñà‚ñã        | 3/18 [01:08<05:36, 22.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: arms_technology_data.pdf (259 chunks)\n",
      "‚úì Processed CSV: army-total-world.csv (9 examples)\n",
      "‚úì Processed CSV: atomic-weapons.csv (0 examples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  39%|‚ñà‚ñà‚ñà‚ñâ      | 7/18 [01:09<01:05,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: CNAS-Defense-Technology-Strategy-2.pdf (142 chunks)\n",
      "‚úì Processed PDF: Codebook1.0.pdf (64 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 8/18 [01:12<00:50,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: CowWarList.pdf (2 chunks)\n",
      "‚úó Error processing cow_arms_tech_long.csv: 'utf-8' codec can't decode byte 0xdf in position 256: invalid continuation byte\n",
      "‚úó Error processing cow_arms_tech_wide.csv: 'utf-8' codec can't decode byte 0xd1 in position 37: invalid continuation byte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 11/18 [01:16<00:21,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: CoW_codebook.pdf (364 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 12/18 [02:33<01:47, 17.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed CSV: DCAD-v1.0-dyadic.csv (0 examples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 13/18 [02:33<01:09, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed CSV: DCAD-v1.0-main.csv (0 examples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 14/18 [02:40<00:49, 12.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: dictionary.pdf (1874 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 16/18 [02:56<00:19,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: Farrow-artillery.pdf (713 chunks)\n",
      "‚úì Processed PDF: JDET-Intelligence-Centre.pdf (33 chunks)\n",
      "‚úì Processed CSV: List_of_countries_by_number_of_military_and_paramilitary_personnel_2023.csv (18 examples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing all dataset files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [02:57<00:00,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Processed PDF: RPsfDIACoEs06032025.pdf (37 chunks)\n",
      "\n",
      "üìä Complete Dataset Processing Summary:\n",
      "   Total files: 18\n",
      "   PDF files: 11\n",
      "   CSV files: 7\n",
      "   TXT files: 0\n",
      "   JSON files: 0\n",
      "   Training examples: 8,059\n",
      "\n",
      "üéØ Complete dataset ready for training!\n",
      "   Training examples: 8,059\n",
      "   Processed data saved to 'complete_defense_training_data.json'\n",
      "\n",
      "üìù Sample training examples:\n",
      "   Example 1: Defense Document (20140808_MOD_Acronyms_and_Abbreviations.pdf): Acronym¬† Long¬†Title¬† 1ACC¬†¬† No.¬†1¬†Air¬†Control¬†Centre¬† 1SL¬† First¬†Sea¬†Lord¬† 200D¬† Second¬†OOD¬† 200W¬† Second¬†00W¬† 2C¬† Second¬†Customer ¬† 2C¬†...\n",
      "   Example 2: Defense Document (20140808_MOD_Acronyms_and_Abbreviations.pdf): AC¬†&¬†CS¬† Air¬†Command ¬†&¬†Control¬†Systems¬† Ac¬†Comd¬† Aircraft¬†Commander ¬† Ac¬†Tech¬† Aircraft¬†Technician ¬† AC(H)¬† Aircraft¬†Controller ¬†(Helic...\n",
      "   Example 3: Defense Document (20140808_MOD_Acronyms_and_Abbreviations.pdf): AFMM¬†(M)¬† Assistant¬†Functional ¬†Machinery ¬†Manager¬†(Maintenance) ¬† AFMM¬†(P)¬† Assistant¬†Functional ¬†Machinery ¬†Manager¬†(Proc)¬† AFMM(PROC)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process your complete 96MB defense dataset (PDF, CSV, TXT, JSON)\n",
    "if os.path.exists(\"defense_dataset\"):\n",
    "    dataset_content, processing_stats = process_complete_defense_dataset(\"defense_dataset\")\n",
    "    \n",
    "    if dataset_content:\n",
    "        print(f\"\\nüéØ Complete dataset ready for training!\")\n",
    "        print(f\"   Training examples: {len(dataset_content):,}\")\n",
    "        \n",
    "        # Save processed data\n",
    "        with open('complete_defense_training_data.json', 'w') as f:\n",
    "            json.dump(dataset_content, f, indent=2)\n",
    "        \n",
    "        print(\"   Processed data saved to 'complete_defense_training_data.json'\")\n",
    "        \n",
    "        # Show samples from different sources\n",
    "        print(f\"\\nüìù Sample training examples:\")\n",
    "        for i, example in enumerate(dataset_content[:3]):\n",
    "            print(f\"   Example {i+1}: {example[:200]}...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No training data generated. Please check your files.\")\n",
    "else:\n",
    "    print(\"‚ùå Please create 'defense_dataset' folder and add your files!\")\n",
    "    dataset_content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a944b954-7bdb-49f6-9d8f-f477239eeb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e32db227b0348ec944557aa56e515f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training dataset created:\n",
      "   Original examples: 8,059\n",
      "   Tokenized examples: 8,059\n"
     ]
    }
   ],
   "source": [
    "def tokenize_defense_data(examples, tokenizer, max_length=512):\n",
    "    \"\"\"Tokenize the defense training data\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Create training dataset from processed content\n",
    "if dataset_content:\n",
    "    # Create Hugging Face dataset\n",
    "    train_dataset = Dataset.from_dict({\"text\": dataset_content})\n",
    "    \n",
    "    # Tokenize dataset\n",
    "    tokenized_dataset = train_dataset.map(\n",
    "        lambda x: tokenize_defense_data(x, defense_gpt.tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Training dataset created:\")\n",
    "    print(f\"   Original examples: {len(dataset_content):,}\")\n",
    "    print(f\"   Tokenized examples: {len(tokenized_dataset):,}\")\n",
    "    \n",
    "else:\n",
    "    tokenized_dataset = None\n",
    "    print(\"‚ùå No dataset to tokenize. Please add files to defense_dataset folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05199a7a-dd8d-4428-8437-5efb79bafbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Training configuration ready for complete 96MB dataset with PDFs\n"
     ]
    }
   ],
   "source": [
    "def setup_complete_training_configuration():\n",
    "    \"\"\"Optimized training configuration for complete 96MB defense dataset\"\"\"\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=\"./defense_complete_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        \n",
    "        # Epochs - optimal for 96MB dataset with PDFs\n",
    "        num_train_epochs=4,\n",
    "        \n",
    "        # Batch size optimized for RTX 4050\n",
    "        per_device_train_batch_size=3,  # Slightly smaller for PDF processing\n",
    "        gradient_accumulation_steps=5,  # Effective batch size of 15\n",
    "        \n",
    "        # Learning rate for domain-specific fine-tuning\n",
    "        learning_rate=3e-5,\n",
    "        \n",
    "        # Optimization settings\n",
    "        warmup_steps=250,\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # Memory and performance\n",
    "        fp16=True,  # Essential for RTX 4050\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        # Monitoring and saving\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        \n",
    "        # Regularization\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        # Advanced settings\n",
    "        remove_unused_columns=False,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "# Setup training configuration\n",
    "training_args = setup_complete_training_configuration()\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=defense_gpt.tokenizer,\n",
    "    mlm=False,  # GPT-2 uses causal LM\n",
    ")\n",
    "\n",
    "print(\"üîß Training configuration ready for complete 96MB dataset with PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175ba23b-3a37-40c2-867f-668344a8a79b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajinkya\\AppData\\Local\\Temp\\ipykernel_7744\\684187671.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting complete defense encyclopedia training...\n",
      "   Training examples: 8,059\n",
      "   Data sources: PDFs, CSVs, TXTs, JSONs\n",
      "   Expected time: 3-5 hours on RTX 4050\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2152' max='2152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2152/2152 5:26:06, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.164100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.165800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.040900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.046300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.055400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.933700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.995500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Complete training finished and model saved!\n"
     ]
    }
   ],
   "source": [
    "def train_complete_defense_encyclopedia():\n",
    "    \"\"\"Train the complete defense encyclopedia model\"\"\"\n",
    "    if tokenized_dataset is None:\n",
    "        print(\"‚ùå No training dataset available!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=defense_gpt.model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        tokenizer=defense_gpt.tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"üöÄ Starting complete defense encyclopedia training...\")\n",
    "    print(f\"   Training examples: {len(tokenized_dataset):,}\")\n",
    "    print(f\"   Data sources: PDFs, CSVs, TXTs, JSONs\")\n",
    "    print(f\"   Expected time: 3-5 hours on RTX 4050\")\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the trained model\n",
    "    trainer.save_model(\"./defense_encyclopedia_complete\")\n",
    "    defense_gpt.tokenizer.save_pretrained(\"./defense_encyclopedia_complete\")\n",
    "    \n",
    "    print(\"‚úÖ Complete training finished and model saved!\")\n",
    "    return trainer\n",
    "\n",
    "# Train the model if dataset is ready\n",
    "if tokenized_dataset:\n",
    "    trained_model = train_complete_defense_encyclopedia()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping training - no dataset available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de185cf2-c1e6-4641-9f97-e9c523f4c684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete trained defense encyclopedia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Testing Complete Defense Encyclopedia:\n",
      "============================================================\n",
      "\n",
      "‚ùì Query: What is a ballistic missile?\n",
      "ü§ñ Response: A ballistic rocket is primarily a defense against enemy missile threats. The main use ballistic missiles is to attack enemy defense forces and the main component of this defense is defense of the strategic area. A defense that is defended against an attack by ballistic rockets will produce a significant boost to national defense. See also ballistic bombing. defense; ballistic defense, also defense strategy. (JP 4-01.7) ballistic cruise missile -A ballistic ballistic weapon with a payload of up to six tons. Also called BSLM. called BM. ballistic trajectory -(*) 1. In naval mine warfare, the path the projectile travels through an area to provide a target for a missile attack. 2. An angle of attack on the weapon. 3. 1.-A line of departure for the ballistic launcher. 4. For surface-to-surface ballistic projectiles, a line connecting the launching of a rocket and its launch system. 5. To launch an aircraft. 6. to launch a ship. 7. a. D-day. 8. Degrees of freedom. 9. Number of revolutions per minute. 10. Standard deviation. 11. Minimum Earth orbit -See Earth's orbit. 12. Direction of\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Query: Explain radar technology\n",
      "ü§ñ Response: Radar technology is a broad term that encompasses all the technologies used to produce, transmit, and receive information. It encompasses both radar and radio communications. The United States has a radar system, but the United Kingdom has no such system. See also  radar; radio. (JP 3-09.3)   2. Radar systems that produce no detectable information, such as laser or  laser-guided weapons. 3. A system that emits no visible or detectable  information on a target. 4. An automatic system which fires either laser, laser guided or non-laser guided. 5. Systems that emit no information or emit a non visible  signal. 6. Electronic or manual means to detect, detect and/or destroy electronic devices. 7. Automatic weapons, missiles, mine warfare, or other weapons that have a firing mechanism which emits a  non detectable signal on target or on its target-targeted portion. 8. In artillery and naval gunfire warfare (including the  use of laser and laserguided weapon systems), a system  that fires the laser seeker or laser guidance. 9. Weapons which have been fired. 10. All types of electronic or electronic warfare.  11. Armed\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Query: Define electronic warfare\n",
      "ü§ñ Response: This is the definition of electronic deception. See also electronic control. (JP 4-09)  electronic attack ‚Äî The act of destroying, destroying or intercepting, either directly or by electronic means, the electronic  communications systems that are designed to provide a means of communication. Also called EAT. e.g., a radio or television system, a receiver, or a system for  controlling a ship or aircraft. 3. To damage, destroy, deface, obstruct, etc. electromagnetic spectrum. 4. An electromagnetic warfare  warfare area. 5. In naval mine warfare, an area of electromagnetic  radiation which is controlled by the use of an electromagnetic command  system. 6. Electronic warfare. The area in which an electronic defense is  conducted. 7. A term applied to electronic, biological, and radiological warfare activities. 8. Offensive electromagnetic operations. 9. Operations designed for the prevention, suppression,  or evasion of a hostile attack. 10. Operational elements of warfare which include, but are not limited to, electronic weapons, ionizing radiation, acoustic warfare -(*)  electromagnetic, electromagnetic control, control devices, electro-optical warfare;  acoustic and acoustic-\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Query: What is military strategy?\n",
      "ü§ñ Response: Military strategy is the study of military forces and their responses to the situation. It is characterized by an evaluation of the military situation and a systematic review of all available information to develop strategic plans. The strategic planning process is conducted by a unit or force commander with the joint force to determine the best course of action. 2. Military Strategy   The main objective of Military Policy is to provide a framework for planning and executing military operations.  4. Strategy for the Military Environment  5. Doctrine of War  6. Development of Joint Staff Doctrine  1. Structure of Operations  2 Document Type: Summary of Terms and Definitions  Strategic Planning Process  JOPES Publication Number: 1280.01  Terms of Definitions Development Phase:  In August 2002, Joint Chiefs of Staff Chairman ofthe Joint Doctrine was established to coordinate and guide the development of joint doctrine. This Joint doctrine is intended to be a common set of concepts and to guide commanders in developing and implementing their joint strategy. Also called JPD. Joint Planning Guidance Document. (JPGD-1022.pdf)  Development Process for Joint Force Doctrine 1  A brief outline of doctrine and the concepts that govern the Joint force\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Query: Describe air defense systems\n",
      "ü§ñ Response: The air defenses are air capabilities which are required to execute all air threats. The Air Defense Component will employ air weapons and/or air operations to ensure the protection of a particular area. Also called AAD. See also air offensive air campaign. (JP 2-0) air superiority -The degree of air mobility. air supremacy -(*) 1. A measure of the military superiority of an air system over a non-air threats -See air dominance. 2. In naval warfare, the degree to which a naval ship can operate at all times in a given area without being exposed to any air attack. 3. An air strategy. 4. Air superiority over enemy submarines is based on air forces. 5. Offensive air operation -1. That part of operations in which the air is directed by a commander or by an observer. Usually limited to a single air target. Normally, in air attacks, only a small number of targets are selected. Examples include a target on the beach in the morning, a beachhead in early morning. bombing range, or a specific target at night. bomber -A bomber with a range greater than 500,000 feet. also called BM. attack -An\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Query: What is cyber warfare?\n",
      "ü§ñ Response: Cyber defense is the defense of the nation through the use of digital technologies and systems. Cyber intelligence is used to disrupt, neutralize, and deny adversary information and techniques. These technologies are used in a variety of ways. Examples include espionage, sabotage, deception, or sabotage. In addition, cyber defense has been used for many military and paramilitary operations. Information security is also an integral part of national security. 2. The protection of critical infrastructure and its resources. 3. the protection and security of all civil and military personnel. 4. military forces and their equipment. 5. national defense. See also cyberspace. cybertaics; cybersecurity. (JP 3-18)   cybereliction -(*) The interdependent nature of a cyberespionage operation. Also called C2. C3. A major cybaton. attack. c. An attack on a key cypoint or cybilibar. d. Attack on an infrastructure. e. Defeated cybors. f. Targeted attacks on key infrastructure or key nodes. g. Damage to key systems, e..g. damage to critical systems to the infrastructure, etc.\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Query: Explain stealth technology\n",
      "ü§ñ Response: The stealth technologies in the technology list are stealthed against, to prevent the detection of enemy infiltration and to provide stealth superiority over conventional weapons. Also called  SHIFT. The concept of stealth is not new. First introduced in WWII, it was used to disrupt enemy reconnaissance, sabotage, and sabotage. In the 1960s, stealth was invented for the first time in space. 2. An air or sea attack or counterattack on an enemy surface. 3. A counterair attack on a surface target. 4. Attack on the surface of a target by a ground or air unit. 5. (JP 3-0) A surface or space attack. 6. Airborne aircraft landing on or near an isolated surface 4-wheeler aircraft. 7. Aircraft landing at an airfield. 8. Assault rifle -(*) A rifle used by infantry to cause casualties. 9. Automatic machine gun -A machinegun which fires an automatic firing mechanism. 10. Smoothbore field gun. 11. Breechloading gun firing. 12. Recoil mechanism for artillery. 13. Machine gun designed to penetrate enemy armor. 14. Antiarmor missile -See  antimateriel. 15. Medium-range\n",
      "----------------------------------------\n",
      "\n",
      "‚ùì Query: What are guided munitions?\n",
      "ü§ñ Response: A guided missile is the guided munition that is fired from the weapon system. The seeker or receiver, by which the seeker is directed at the surface, and the receiver directs the missile in a manner to deliver the munitions. See also guided. 2. In artillery, a projectile whose trajectory is guided, aiming point. 3. A device of the same type or type of projectile with the aiming mechanism, which is mounted on the head. 4. An instrument used for the purpose of determining the direction of a point by a guidance system, in artillery. 5. (DOD only) A projectile which fires a missile at a target. 6. Explosive or nuclear material in its entirety. 7. Alloys, materials, or projectiles in the manufacture, use, transport, manufacture for use in military or paramilitary purposes, either in demolition or in war. Also called HUMINT. ammunition. 8. Ammunition, including but not limited to ammunition for artillery and ammunition parts. 9. Items used in battle or naval mine warfare. 10. Weapons and munitions of mass destruction. 11. Military Sealift Command -A command, control, production, transportation, supplies, facilities, ammunition, weapons,\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_complete_defense_encyclopedia():\n",
    "    \"\"\"Test the trained complete defense encyclopedia\"\"\"\n",
    "    # Load the trained model if it exists\n",
    "    if os.path.exists(\"./defense_encyclopedia_complete\"):\n",
    "        print(\"Loading complete trained defense encyclopedia...\")\n",
    "        defense_gpt.model = GPT2LMHeadModel.from_pretrained(\"./defense_encyclopedia_complete\")\n",
    "        defense_gpt.tokenizer = GPT2Tokenizer.from_pretrained(\"./defense_encyclopedia_complete\")\n",
    "        defense_gpt.model.to(device)\n",
    "    \n",
    "    # Test queries for defense topics\n",
    "    test_queries = [\n",
    "        \"What is a ballistic missile?\",\n",
    "        \"Explain radar technology\",\n",
    "        \"Define electronic warfare\",\n",
    "        \"What is military strategy?\",\n",
    "        \"Describe air defense systems\",\n",
    "        \"What is cyber warfare?\",\n",
    "        \"Explain stealth technology\",\n",
    "        \"What are guided munitions?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üéØ Testing Complete Defense Encyclopedia:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\n‚ùì Query: {query}\")\n",
    "        response = defense_gpt.generate_defense_response(query, max_length=250)\n",
    "        print(f\"ü§ñ Response: {response}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Test the trained model\n",
    "test_complete_defense_encyclopedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa324cb-743b-4e59-933b-933c26cd4577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Run this to start interactive testing:\n",
      "interactive_complete_defense_encyclopedia()\n"
     ]
    }
   ],
   "source": [
    "def interactive_complete_defense_encyclopedia():\n",
    "    \"\"\"Interactive interface for your complete defense encyclopedia\"\"\"\n",
    "    print(\"üõ°Ô∏è Complete Defense Encyclopedia - Interactive Mode\")\n",
    "    print(\"Ask questions about defense, military, and security topics\")\n",
    "    print(\"Trained on PDFs, CSVs, TXTs, and JSON files\")\n",
    "    print(\"Type 'quit' to exit, 'stats' to see dataset statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nüîç Ask about defense topics: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        elif query.lower() == 'stats':\n",
    "            if 'processing_stats' in locals():\n",
    "                print(f\"\\nüìä Complete Dataset Statistics:\")\n",
    "                for key, value in processing_stats.items():\n",
    "                    print(f\"   {key}: {value}\")\n",
    "            continue\n",
    "        \n",
    "        if query:\n",
    "            try:\n",
    "                response = defense_gpt.generate_defense_response(query, max_length=300)\n",
    "                print(f\"\\nüõ°Ô∏è Defense Encyclopedia:\")\n",
    "                print(response)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "        else:\n",
    "            print(\"Please enter a valid query.\")\n",
    "\n",
    "# Start interactive mode\n",
    "print(\"üìù Run this to start interactive testing:\")\n",
    "print(\"interactive_complete_defense_encyclopedia()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "629fea3d-0d4d-4ffc-b01f-5240d235d606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Complete Dataset Validation Results:\n",
      "   üìÅ Files found: 18\n",
      "   üíæ Total size: 96.0 MB\n",
      "   üìù File formats:\n",
      "      .pdf: 11 files\n",
      "      .csv: 7 files\n",
      "   üéØ Status: ‚úÖ Perfect for training!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_complete_96mb_dataset():\n",
    "    \"\"\"Validate your complete 96MB dataset with PDFs\"\"\"\n",
    "    \n",
    "    if not os.path.exists(\"defense_dataset\"):\n",
    "        print(\"‚ùå Defense dataset folder not found!\")\n",
    "        return False\n",
    "    \n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    format_count = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(\"defense_dataset\"):\n",
    "        for file in files:\n",
    "            if file.endswith(('.pdf', '.csv', '.txt', '.json', '.jsonl')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                total_size += size\n",
    "                file_count += 1\n",
    "                \n",
    "                ext = os.path.splitext(file)[1].lower()\n",
    "                format_count[ext] = format_count.get(ext, 0) + 1\n",
    "    \n",
    "    size_mb = total_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"üìä Complete Dataset Validation Results:\")\n",
    "    print(f\"   üìÅ Files found: {file_count}\")\n",
    "    print(f\"   üíæ Total size: {size_mb:.1f} MB\")\n",
    "    print(f\"   üìù File formats:\")\n",
    "    for ext, count in format_count.items():\n",
    "        print(f\"      {ext}: {count} files\")\n",
    "    \n",
    "    status = \"‚úÖ Perfect for training!\" if 80 <= size_mb <= 120 else \"‚ö†Ô∏è Size outside optimal range\"\n",
    "    print(f\"   üéØ Status: {status}\")\n",
    "    \n",
    "    return 80 <= size_mb <= 120\n",
    "\n",
    "# Validate your complete dataset\n",
    "validate_complete_96mb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3fb64-d277-4d09-b52b-92815ad65e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
